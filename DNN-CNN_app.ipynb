{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import keras\\nimport tensorflow\\nkeras.__version__'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import keras\n",
    "import tensorflow\n",
    "keras.__version__\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow.__version__'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"tensorflow.__version__\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, render_template\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "#counting no. of numericals in the url and adding a column to the dataframe   1\n",
    "def count_num(url):\n",
    "    num = len( re.findall('[\\d]', str(url)) )\n",
    "    return num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It was found that - . / ? = _ are usually found in the Phishing urls, \n",
    "#so new columns are created showing the count of each of the special character count in each url\n",
    "\n",
    "#Count of the usually found special characters other than above mentioned characters \n",
    "#are generalized into a column called miscellaneous character\n",
    "\n",
    "#This is done by using REGULAR EXPRESSION concept\n",
    "\n",
    "def count_misc_ch(url): #2\n",
    "    sc=0\n",
    "    sc = sc+len( re.findall(\"'\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\\\\\\\\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"%\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"&\", str(url)) )\n",
    "    sc = sc+len( re.findall(\";\", str(url)) )\n",
    "    sc = sc+len( re.findall(\":\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"#\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"~\", str(url)) )\n",
    "    sc = sc+len( re.findall(\",\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"}\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"{\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\+\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\)\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\(\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"!\", str(url)) )\n",
    "    sc = sc+len( re.findall(\" \", str(url)) )\n",
    "    sc = sc+len( re.findall(\">\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\^\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\$\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\|\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\*\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"\\[\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"//\", str(url)) )\n",
    "    sc = sc+len( re.findall(\"@\", str(url)) )\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_d(url): #3\n",
    "    sc = len( re.findall(\"-\", str(url)) )\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_dot(url): #4\n",
    "    sc = len( re.findall(\"\\.\", str(url)) )\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_slash(url):  #5\n",
    "    sc = len( re.findall(\"/\", str(url)) )\n",
    "    m = len( re.findall(\"//\", str(url)) )\n",
    "    sc=sc-(m*2)\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_qy(url): #6\n",
    "    sc = len( re.findall(\"\\?\", str(url)) )\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_eq(url): #7\n",
    "    sc = len( re.findall(\"=\", str(url)) )\n",
    "    return sc\n",
    "\n",
    "\n",
    "def count_un(url): #8\n",
    "    sc = len( re.findall(\"_\", str(url)) )\n",
    "    return sc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Length of each url is calculated and recorded in a new column called count_len\n",
    "#This feature is important bcz of usually found variations in the length of phishing urls and legitimate urls.\n",
    "#Phishing urls tend to have more length\n",
    "\n",
    "def count_len(url): #9\n",
    "    l = len(url)\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating Domain length and path length. Phishing urls tend to have more length\n",
    "#Domain and path are extracted from the url using a pattern and REGULAR EXPRESSIONS\n",
    "#A general pattern resembling a domain and path in url is defined using which they are extracted from url and length is calculated\n",
    "#As it was not possoble to go through each and every url. If there are any unusual url, those are handled using try except block\n",
    "\n",
    "def domain_len(url):\n",
    "    pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        l = len(m.group(2))\n",
    "    except:\n",
    "        l=0\n",
    "    return l\n",
    "\n",
    "\n",
    "def path_len(url):\n",
    "    pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        l = len(m.group(3))\n",
    "    except:\n",
    "        l=0\n",
    "    return l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inorder to find how many of the urls in the dataframe have protocol or no... \n",
    "# A binary column is created to check the presence of the protocol in the url.\n",
    "#Groups under RE concept is used to do the same\n",
    "\n",
    "def protocol(url):\n",
    "    pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        l = len(m.group(1))\n",
    "        if(l>0):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    except:\n",
    "        return 0\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names={'20minutos',\n",
    " '4shared',\n",
    " 'a8',\n",
    " 'pes',\n",
    " 'pesu',\n",
    " 'pesuacademy',\n",
    " 'pesit',\n",
    " 'abc',\n",
    " 'ieee',\n",
    " 'indianvisaonline.gov.in',             \n",
    " 'about',\n",
    " 'aboutads',\n",
    " 'abril',\n",
    " 'academia',\n",
    " 'addthis',\n",
    " 'addtoany',\n",
    " 'adobe',\n",
    " 'afternic',\n",
    " 'akamaihd',\n",
    " 'alexa',\n",
    " 'alibaba',\n",
    " 'aliexpress',\n",
    " 'allaboutcookies',\n",
    " 'allrecipes',\n",
    " 'amazon',\n",
    " 'ameblo',\n",
    " 'android',\n",
    " 'aol',\n",
    " 'apache',\n",
    " 'apple',\n",
    " 'archive',\n",
    " 'archives',\n",
    " 'arstechnica',\n",
    " 'asus',\n",
    " 'bandcamp',\n",
    " 'bbc',\n",
    " 'behance',\n",
    " 'berkeley',\n",
    " 'biglobe',\n",
    " 'bing',\n",
    " 'bit',\n",
    " 'bitly',\n",
    " 'blackberry',\n",
    " 'blogger',\n",
    " 'bloglovin',\n",
    " 'blogspot',\n",
    " 'bloomberg',\n",
    " 'booking',\n",
    " 'box',\n",
    " 'brandbucket',\n",
    " 'britannica',\n",
    " 'bund',\n",
    " 'businessinsider',\n",
    " 'buydomains',\n",
    " 'buzzfeed',\n",
    " 'cam',\n",
    " 'cambridge',\n",
    " 'canalblog',\n",
    " 'cbc',\n",
    " 'cbslocal',\n",
    " 'cbsnews',\n",
    " 'cdc',\n",
    " 'change',\n",
    " 'channel4',\n",
    " 'chicagotribune',\n",
    " 'cia',\n",
    " 'cisco',\n",
    " 'clickbank',\n",
    " 'cloudflare',\n",
    " 'cmu',\n",
    " 'cnbc',\n",
    " 'cnet',\n",
    " 'cnn',\n",
    " 'columbia',\n",
    " 'cornell',\n",
    " 'coursera',\n",
    " 'cpanel',\n",
    " 'creativecommons',\n",
    " 'dailymail',\n",
    " 'dailymotion',\n",
    " 'dan',\n",
    " 'daum',\n",
    " 'debian',\n",
    " 'deezer',\n",
    " 'dell',\n",
    " 'depositfiles',\n",
    " 'detik',\n",
    " 'digg',\n",
    " 'discord',\n",
    " 'disney',\n",
    " 'disqus',\n",
    " 'doi',\n",
    " 'dot',\n",
    " 'doubleclick',\n",
    " 'dreniq',\n",
    " 'dribbble',\n",
    " 'dropbox',\n",
    " 'dw',\n",
    " 'e-monsite',\n",
    " 'e-recht24',\n",
    " 'ea',\n",
    " 'ebay',\n",
    " 'economist',\n",
    " 'ed',\n",
    " 'eff',\n",
    " 'elmundo',\n",
    " 'elpais',\n",
    " 'elsevier',\n",
    " 'enable-javascript',\n",
    " 'engadget',\n",
    " 'entrepreneur',\n",
    " 'epa',\n",
    " 'europa',\n",
    " 'eventbrite',\n",
    " 'excite',\n",
    " 'express',\n",
    " 'facebook',\n",
    " 'fandom',\n",
    " 'fb',\n",
    " 'fc2',\n",
    " 'feedburner',\n",
    " 'fifa',\n",
    " 'forbes',\n",
    " 'forms',\n",
    " 'fortune',\n",
    " 'foursquare',\n",
    " 'foxnews',\n",
    " 'freepik',\n",
    " 'ft',\n",
    " 'ggpht',\n",
    " 'github',\n",
    " 'gizmodo',\n",
    " 'globo',\n",
    " 'gmail',\n",
    " 'gnu',\n",
    " 'go',\n",
    " 'godaddy',\n",
    " 'gofundme',\n",
    " 'goo',\n",
    " 'goodreads',\n",
    " 'google',\n",
    " 'googleapis',\n",
    " 'googleblog',\n",
    " 'googleusercontent',\n",
    " 'gooyaabitemplates',\n",
    " 'gravatar',\n",
    " 'greenpeace',\n",
    " 'gstatic',\n",
    " 'guardian',\n",
    " 'gutenberg',\n",
    " 'harvard',\n",
    " 'hatena',\n",
    " 'histats',\n",
    " 'hm',\n",
    " 'hollywoodreporter',\n",
    " 'home',\n",
    " 'house',\n",
    " 'howstuffworks',\n",
    " 'hp',\n",
    " 'huawei',\n",
    " 'huffingtonpost',\n",
    " 'huffpost',\n",
    " 'hugedomains',\n",
    " 'ibm',\n",
    " 'icann',\n",
    " 'ieee',\n",
    " 'ietf',\n",
    " 'ig',\n",
    " 'ign',\n",
    " 'ikea',\n",
    " 'imageshack',\n",
    " 'imdb',\n",
    " 'imgur',\n",
    " 'inc',\n",
    " 'independent',\n",
    " 'indiatimes',\n",
    " 'indiegogo',\n",
    " 'insider',\n",
    " 'instagram',\n",
    " 'instructables',\n",
    " 'intel',\n",
    " 'investopedia',\n",
    " 'irs',\n",
    " 'issuu',\n",
    " 'istockphoto',\n",
    " 'iubenda',\n",
    " 'jimdofree',\n",
    " 'jstor',\n",
    " 'kickstarter',\n",
    " 'kinja',\n",
    " 'latimes',\n",
    " 'lefigaro',\n",
    " 'lemonde',\n",
    " 'line',\n",
    " 'linkedin',\n",
    " 'list-manage',\n",
    " 'live',\n",
    " 'livejournal',\n",
    " 'livescience',\n",
    " 'loc',\n",
    " 'lycos',\n",
    " 'marketwatch',\n",
    " 'marriott',\n",
    " 'mashable',\n",
    " 'mediafire',\n",
    " 'medium',\n",
    " 'mega',\n",
    " 'megaupload',\n",
    " 'merriam-webster',\n",
    " 'metro',\n",
    " 'microsoft',\n",
    " 'mirror',\n",
    " 'mit',\n",
    " 'mixcloud',\n",
    " 'mozilla',\n",
    " 'msn',\n",
    " 'myspace',\n",
    " 'mysql',\n",
    " 'mystrikingly',\n",
    " 'namecheap',\n",
    " 'narod',\n",
    " 'nasa',\n",
    " 'nationalgeographic',\n",
    " 'nature',\n",
    " 'naver',\n",
    " 'nbcnews',\n",
    " 'ndtv',\n",
    " 'netflix',\n",
    " 'netlify',\n",
    " 'netvibes',\n",
    " 'networkadvertising',\n",
    " 'newscientist',\n",
    " 'newsweek',\n",
    " 'newyorker',\n",
    " 'nginx',\n",
    " 'nih',\n",
    " 'nikkei',\n",
    " 'noaa',\n",
    " 'npr',\n",
    " 'nvidia',\n",
    " 'nydailynews',\n",
    " 'nypost',\n",
    " 'nytimes',\n",
    " 'oecd',\n",
    " 'office',\n",
    " 'ok',\n",
    " 'opera',\n",
    " 'oracle',\n",
    " 'oup',\n",
    " 'outlook',\n",
    " 'over-blog',\n",
    " 'over-blog-kiwi',\n",
    " 'ovh',\n",
    " 'parallels',\n",
    " 'pastebin',\n",
    " 'paypal',\n",
    " 'pbs',\n",
    " 'pcmag',\n",
    " 'pexels',\n",
    " 'photobucket',\n",
    " 'php',\n",
    " 'pinterest',\n",
    " 'pixabay',\n",
    " 'playstation',\n",
    " 'plesk',\n",
    " 'plos',\n",
    " 'prezi',\n",
    " 'princeton',\n",
    " 'privacyshield',\n",
    " 'prnewswire',\n",
    " 'psu',\n",
    " 'psychologytoday',\n",
    " 'qq',\n",
    " 'quora',\n",
    " 'qz',\n",
    " 'rakuten',\n",
    " 'rambler',\n",
    " 'rapidshare',\n",
    " 'redhat',\n",
    " 'repubblica',\n",
    " 'researchgate',\n",
    " 'reuters',\n",
    " 'reverbnation',\n",
    " 'ria',\n",
    " 'rollingstone',\n",
    " 'rottentomatoes',\n",
    " 'rt',\n",
    " 'samsung',\n",
    " 'sapo',\n",
    " 'sciencedaily',\n",
    " 'sciencedirect',\n",
    " 'sciencemag',\n",
    " 'scientificamerican',\n",
    " 'scribd',\n",
    " 'secureserver',\n",
    " 'sedo',\n",
    " 'sendspace',\n",
    " 'sfgate',\n",
    " 'shopify',\n",
    " 'shutterstock',\n",
    " 'sina',\n",
    " 'skype',\n",
    " 'slate',\n",
    " 'slideshare',\n",
    " 'smh',\n",
    " 'soundcloud',\n",
    " 'spiegel',\n",
    " 'spotify',\n",
    " 'springer',\n",
    " 'sputniknews',\n",
    " 'stackoverflow',\n",
    " 'standard',\n",
    " 'stanford',\n",
    " 'steamcommunity',\n",
    " 'steampowered',\n",
    " 'stuff',\n",
    " 'surveymonkey',\n",
    " 't',\n",
    " 'tabelog',\n",
    " 'target',\n",
    " 'teamviewer',\n",
    " 'techcrunch',\n",
    " 'ted',\n",
    " 'telegram',\n",
    " 'telegraph',\n",
    " 'terra',\n",
    " 'tes',\n",
    " 'theatlantic',\n",
    " 'thedailybeast',\n",
    " 'thefreedictionary',\n",
    " 'theglobeandmail',\n",
    " 'theguardian',\n",
    " 'themeforest',\n",
    " 'thenextweb',\n",
    " 'thestar',\n",
    " 'thesun',\n",
    " 'thetimes',\n",
    " 'theverge',\n",
    " 'time',\n",
    " 'timeweb',\n",
    " 'tinyurl',\n",
    " 'tripadvisor',\n",
    " 'trustpilot',\n",
    " 'twitch',\n",
    " 'twitter',\n",
    " 'ubuntu',\n",
    " 'udemy',\n",
    " 'umich',\n",
    " 'un',\n",
    " 'undeveloped',\n",
    " 'unesco',\n",
    " 'unsplash',\n",
    " 'uol',\n",
    " 'urbandictionary',\n",
    " 'usatoday',\n",
    " 'usgs',\n",
    " 'usnews',\n",
    " 'utexas',\n",
    " 'variety',\n",
    " 'vice',\n",
    " 'viglink',\n",
    " 'vimeo',\n",
    " 'vk',\n",
    " 'vkontakte',\n",
    " 'w3',\n",
    " 'wa',\n",
    " 'walmart',\n",
    " 'washington',\n",
    " 'washingtonpost',\n",
    " 'webmd',\n",
    " 'weebly',\n",
    " 'weforum',\n",
    " 'weibo',\n",
    " 'welt',\n",
    " 'whatsapp',\n",
    " 'whitehouse',\n",
    " 'who',\n",
    " 'wikia',\n",
    " 'wikihow',\n",
    " 'wikimedia',\n",
    " 'wikipedia',\n",
    " 'wiktionary',\n",
    " 'wiley',\n",
    " 'windows',\n",
    " 'wired',\n",
    " 'wisc',\n",
    " 'wix',\n",
    " 'wn',\n",
    " 'wordpress',\n",
    " 'worldbank',\n",
    " 'wp',\n",
    " 'wsj',\n",
    " 'xbox',\n",
    " 'xing',\n",
    " 'xinhuanet',\n",
    " 'yadi',\n",
    " 'yahoo',\n",
    " 'yale',\n",
    " 'yandex',\n",
    " 'yelp',\n",
    " 'youronlinechoices',\n",
    " 'youtu',\n",
    " 'youtube',\n",
    " 'ytimg',\n",
    " 'zdnet',\n",
    " 'zeit',\n",
    " 'zendesk',\n",
    " 'ziddu','flipkart','google','yahoo','ebay','passportindia','ieee','amazon','godaddy','alibaba','zappos','aliexpress','rakuten','walmart','target','geeksforgeeks','leetcode','makemytrip','bookmyshow','zomato','swiggy','impartus','axisbank','icicibank','sciencedirect','microsoft'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [\"www.indianvisaonline.gov.in/evisa/tvoa.html\",\"indianvisaonline.gov.in/evisa/tvoa.html\",\"www.indianvisaonline.gov.in\",\"indianvisaonline.gov.in\",\"www.indianvisaonline.gov.in/evisa/images/SampleForm.pdf\",\"indianvisaonline.gov.in/evisa/images/SampleForm.pdf\",\"indianvisaonline.gov.in/evisa/PaymentCheck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "brand_names = set(brand_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_ld={'abcnews',\n",
    " 'accounts',\n",
    " 'adssettings',\n",
    " 'answers',\n",
    " 'blog',\n",
    " 'books',\n",
    " 'bp',\n",
    " 'bp1',\n",
    " 'bp2',\n",
    " 'bp3',\n",
    " 'calendar',\n",
    " 'code',\n",
    " 'de',\n",
    " 'developers',\n",
    " 'docs',\n",
    " 'draft',\n",
    " 'drive',\n",
    " 'en',\n",
    " 'es',\n",
    " 'espn',\n",
    " 'feedproxy',\n",
    " 'files',\n",
    " 'finance',\n",
    " 'fr',\n",
    " 'get',\n",
    " 'groups',\n",
    " 'id',\n",
    " 'ipv4',\n",
    " 'it',\n",
    " 'ja',\n",
    " 'm',\n",
    " 'mail',\n",
    " 'maps',\n",
    " 'marketingplatform',\n",
    " 'my',\n",
    " 'myaccount',\n",
    " 'news',\n",
    " 'photos',\n",
    " 'picasa',\n",
    " 'picasaweb',\n",
    " 'pl',\n",
    " 'play',\n",
    " 'plus',\n",
    " 'policies',\n",
    " 'pt',\n",
    " 'ru',\n",
    " 'search',\n",
    " 'sites',\n",
    " 'sports',\n",
    " 'storage',\n",
    " 'support',\n",
    " 'tools',\n",
    " 'translate',\n",
    " 'www'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_ld={'ac.uk',\n",
    " 'app',\n",
    " 'be',\n",
    " 'ca',\n",
    " 'co',\n",
    " 'co.id',\n",
    " 'co.in',\n",
    " 'co.jp',\n",
    " 'co.nz',\n",
    " 'co.uk',\n",
    " 'com',\n",
    " 'com.au',\n",
    " 'com.br',\n",
    " 'com.cn',\n",
    " 'com.tw',\n",
    " 'de',\n",
    " 'edu',\n",
    " 'es',\n",
    " 'eu',\n",
    " 'fr',\n",
    " 'gg',\n",
    " 'gl',\n",
    " 'gle',\n",
    " 'gov',\n",
    " 'gov.in'\n",
    " 'gov.uk',\n",
    " 'in',\n",
    " 'info',\n",
    " 'int',\n",
    " 'it',\n",
    " 'jp',\n",
    " 'ly',\n",
    " 'me',\n",
    " 'ne.jp',\n",
    " 'net',\n",
    " 'net.au',\n",
    " 'neustar',\n",
    " 'nl',\n",
    " 'nz',\n",
    " 'org',\n",
    " 'pl',\n",
    " 'pt',\n",
    " 'ru',\n",
    " 'sk',\n",
    " 'tk',\n",
    " 'tv',\n",
    " 'us'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'we created a list of suspicious words like confirm, webscr, login etc. and later compared it with all URLs in our dataset\\nand counted the number of suspicious words in each url because such words are commonly used in phishing urls \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suspicious_words = [\"confirm\", \"account\", \"secure\", \"webscr\", \"login\", \"signin\", \"submit\", \"update\", \"logon\", \"secure\", \"wp\", \"cmd\",\"admin\",\"approval\", \"authority\", \"balance\", \"amount\",'g00gle','yah00','googles','amp','php','index','id','openid','wp-content','aspx','email','battle','cmd','webscr','images','d3']\n",
    "\n",
    "\"\"\"we created a list of suspicious words like confirm, webscr, login etc. and later compared it with all URLs in our dataset\n",
    "and counted the number of suspicious words in each url because such words are commonly used in phishing urls \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "suspicious_words = set(suspicious_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def match(url, string_list):\n",
    "    words = re.findall(r'\\w+', str(url))\n",
    "    return [word for word in words if word in string_list]\n",
    "\n",
    "\n",
    "\"\"\"the function brandCount, checks for match of tokens of the URL with the brand name and if they match the brandCount \n",
    "for that URL is incremented by one.\"\"\"\n",
    "def brandCount(tokens):\n",
    "    l = match(tokens, brand_names)\n",
    "    return len(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the function suspiciousCount, checks for match of tokens of the URL with the suspicious words and if they match the\n",
    "SuspiciousCount for that URL is incremented by one.\"\"\"\n",
    "def suspiciousCount(tokens):\n",
    "    l = match(tokens, suspicious_words)\n",
    "    return len(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_top_ld = ['com','ru','org','net','info','biz','online','blog','travel','web','app','edu','gov','uk','ca','jp','fr','au','us','de','ch','it','nl','se','no','es','mil']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_top_ld=set(common_top_ld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"the function common_top_ld_Count, checks for match of tokens of the URL with the common top level domains and if they match \n",
    "the common_top_ld_Count for that URL is incremented by one.\"\"\"\n",
    "def common_top_ld_Count(tokens):\n",
    "    l = match(tokens, common_top_ld)\n",
    "    return len(l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the tokens of domain\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "\n",
    "pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'\n",
    "def dom(url):\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        return (m.group(2))#inorder to get the domain of the urls the 2nd subgroup of the pattern has to be taken\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_tok(tok): #counts the number of tokens in the domain\n",
    "    if tok=='':\n",
    "        return 0\n",
    "    else:\n",
    "        l=[]\n",
    "        c=0\n",
    "        l = tok.split()\n",
    "        c = len(l)\n",
    "        return c\n",
    "\n",
    "def lar_tok(tok): # finds the lenght of the largest token present in the domain of a url \n",
    "    l=[]\n",
    "    ml = 0\n",
    "    tl = 0\n",
    "    text = tok\n",
    "    l = text.split()\n",
    "    c = len(l)\n",
    "    for t in l:\n",
    "        tl = tl + len(t)\n",
    "        if (ml<len(t)):\n",
    "            ml = len(t)\n",
    "    return ml\n",
    "\n",
    "def avg_dom_tok_len(tok): # finds the average token length in a url \n",
    "    ml = 0\n",
    "    tl = 0\n",
    "    text = tok\n",
    "    l = text.split()\n",
    "    c = len(l)\n",
    "    for t in l:\n",
    "        tl = tl + len(t)\n",
    "        if (ml<len(t)):\n",
    "            ml = len(t)\n",
    "    if c==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (tl/c)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'\n",
    "def path(url):\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        return (m.group(3)) #takes the 3rd subgroup from the above pattern which is the path incase of urls\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def avg_path_tok_len(tok): #finding the average lenth of path \n",
    "    ml = 0\n",
    "    tl = 0\n",
    "    text = tok\n",
    "    l = text.split()\n",
    "    c = len(l)\n",
    "    for t in l:\n",
    "        tl = tl + len(t)\n",
    "        if (ml<len(t)):\n",
    "            ml = len(t)\n",
    "    if c==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (tl/c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)' #finding the count of directories found in the url .\n",
    "        \n",
    "def dire_count(url):\n",
    "    try:\n",
    "        m=re.match(pattern,url)\n",
    "        text=m.group(3) # the path subgroup is taken since it contains the directories and the file. \n",
    "        cou=len((re.findall(r'[^/]+/', text))) #regular expression to find the number of directories in a url\n",
    "        return cou\n",
    "    except:\n",
    "        #print(i)\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the number of digits present in domain\n",
    "pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)'         \n",
    "def domain_digit_count(url):\n",
    "    try:\n",
    "        dom=re.match(pattern,url)\n",
    "        dom=dom.group(2)\n",
    "        dom_count=len(re.findall('\\d', dom)) #regular expression to find the number of digits \n",
    "        return dom_count\n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to check the presence of http/https in domain\n",
    "pattern=r'(ftps?://|https?://)?([^/]*\\.?[^/]+\\.[^/]+)(/*.*)' \n",
    "def presence_http(url):  \n",
    "    try:\n",
    "        http=re.match(pattern,url)\n",
    "        http=http.group(2)\n",
    "        if(re.findall(\"http|https\", http)) :#regular expression to find http/https\n",
    "            return 1\n",
    "        else:\n",
    "            return 0    \n",
    "    except:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p = Flask(__name__)\\n@app.route(\\'/\\',methods=[\\'GET\\', \\'POST\\'])\\ndef index():\\n    if request.method==\\'GET\\':\\n        return render_template(\"index.html\")\\n\\n    if request.method==\\'POST\\':\\n        in_url = request.form[\\'Search\\']\\n        in_url=str(in_url)\\n        return render_template(\"index.html\", in_url=in_url)'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"p = Flask(__name__)\n",
    "@app.route('/',methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method=='GET':\n",
    "        return render_template(\"index.html\")\n",
    "\n",
    "    if request.method=='POST':\n",
    "        in_url = request.form['Search']\n",
    "        in_url=str(in_url)\n",
    "        return render_template(\"index.html\", in_url=in_url)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from requests.exceptions import ConnectionError\n",
    "app = Flask(__name__)\n",
    "    \n",
    "@app.route('/',methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method=='GET':\n",
    "        return render_template(\"index.html\")\n",
    "\n",
    "    if request.method=='POST':\n",
    "        url = request.form['Search']\n",
    "        m =\"URL entered: \"\n",
    "        pattern =\"(ftps?://|https?://)?([^/]*\\\\.?[^/]+\\\\.[^/]+)(/*.*)\"\n",
    "        if(not(re.match(pattern,url))):\n",
    "            a = \"The URL format is wrong.\"\n",
    "            return render_template(\"index.html\", a=a,url=url,m=m)\n",
    "\n",
    "\n",
    "        try:\n",
    "            pat = \"^((http|https|ftp):\\/\\/)\"\n",
    "            if (not(re.match(pat,url))):\n",
    "                url = \"http://\" + url\n",
    "            req = requests.get(url)\n",
    "        except ConnectionError:\n",
    "            x = 'NO'\n",
    "        else:\n",
    "            x = 'YES'\n",
    "        '''if x == 'Website does not exist,':\n",
    "            return render_template(\"index.html\",x=x,url=url,m=m)'''\n",
    "        \n",
    "        def remove_proto(url):\n",
    "            url1 = url\n",
    "            pat = re.compile('(ftps?://|https?://)?([^/]*\\\\.?[^/]+\\\\.[^/]+)(/*.*)')\n",
    "            try:\n",
    "                mat = re.match(pat,url1)\n",
    "                if (mat.group(1)):\n",
    "                    url1 = url1.replace(mat.group(1), '')\n",
    "                    #print(url)\n",
    "                else:\n",
    "                    return url\n",
    "            except:\n",
    "                return url\n",
    "            return url1\n",
    "        url1 = remove_proto(url)\n",
    "        \n",
    "        if(url1 in list1):\n",
    "            prediction = \"Legitimate URL\"\n",
    "            return render_template(\"index.html\", prediction=prediction,url=url, x=x,m=m) \n",
    "            \n",
    "        #url=str(url)\n",
    "        real_ip = pd.DataFrame(columns = ['numericals', 'misc_ch', 'dash', 'dot', 'slash', 'quest', 'equal',\n",
    "               'underscore', 'length_url', 'domain_length', 'path_length',\n",
    "               'brandCount', 'suspiciousCount', 'comm_tldCount', 'no_tok_dom',\n",
    "               'dom_large_tok_len', 'avg_dom_tok_len', 'is_IP', 'avg_path_tok_len',\n",
    "               'dir_count', 'domain_digit_count', 'presence_http'])\n",
    "\n",
    "        mip = list()\n",
    "\n",
    "        numericals = count_num(url1)\n",
    "        misc_ch = count_misc_ch(url1)\n",
    "        dash = count_d(url1)\n",
    "        dot = count_dot(url1)\n",
    "        slash = count_slash(url1)\n",
    "        quest = count_qy(url1)\n",
    "        equal = count_eq(url1)\n",
    "        under = count_un(url1)\n",
    "        length = count_len(url1)\n",
    "        dom_len = domain_len(url1)\n",
    "        pat_len = path_len(url1)\n",
    "        dom_digit=domain_digit_count(url1)\n",
    "        http_or_https=presence_http(url1)\n",
    "\n",
    "            #convert text to tokens\n",
    "        tok = url1.replace(\"[^a-zA-Z#]\", \" \")\n",
    "        tok = tok.replace(\"\\s+\", \" \")\n",
    "            #tok = tok.fillna(\"\")\n",
    "            #if tok =\n",
    "\n",
    "        brand_count = brandCount(tok)\n",
    "        susp_words = suspiciousCount(tok)\n",
    "        no_tld = common_top_ld_Count(tok)\n",
    "\n",
    "        dom_url = dom(url1)\n",
    "\n",
    "        dom_tok = str(dom_url).replace(\"[^a-zA-Z#]\", \" \")\n",
    "        dom_tok = dom_tok.replace(\"\\s+\", \" \")\n",
    "            #dom_tok = dom_tok.fillna(\"\")\n",
    "\n",
    "        no_tok_dom = no_tok(dom_tok)\n",
    "\n",
    "        if(no_tok_dom == 0):\n",
    "            is_ip = 1\n",
    "        else:\n",
    "            is_ip = 0\n",
    "\n",
    "        dom_lar_tok = lar_tok(dom_tok)\n",
    "        avg_dom_tok = avg_dom_tok_len(dom_tok)\n",
    "\n",
    "            #df['is_IP'] = df['no_tok_dom'].apply(lambda row: 1 if row==0 else 0)\n",
    "\n",
    "        path_tok  = path(url1)\n",
    "        path_tok = str(path_tok).replace(\"[^a-zA-Z#]\", \" \")\n",
    "        path_tok = path_tok.replace(\"\\s+\", \" \")\n",
    "            #path_tok = path_tok.fillna(\"\")\n",
    "\n",
    "        avg_path_tok = avg_path_tok_len(path_tok)\n",
    "        no_dir = dire_count(url1)\n",
    "\n",
    "        mip.append(numericals)\n",
    "        mip.append(misc_ch)\n",
    "        mip.append(dash)\n",
    "        mip.append(dot)\n",
    "        mip.append(slash)\n",
    "        mip.append(quest)\n",
    "        mip.append(equal)\n",
    "        mip.append(under)\n",
    "        mip.append(length)\n",
    "        mip.append(dom_len)\n",
    "        mip.append(pat_len)\n",
    "        mip.append(brand_count)\n",
    "        mip.append(susp_words)\n",
    "        mip.append(no_tld)\n",
    "        mip.append(no_tok_dom)\n",
    "        mip.append(dom_lar_tok)\n",
    "        mip.append(avg_dom_tok)\n",
    "        mip.append(is_ip)\n",
    "        mip.append(avg_path_tok)\n",
    "        mip.append(no_dir)\n",
    "        mip.append(http_or_https)\n",
    "        mip.append(dom_digit)\n",
    "\n",
    "\n",
    "        \n",
    "        real_ip.loc[0] = mip\n",
    "        #LOAD THE ML MODEL\n",
    "        model = load_model('DNNCNN.h5')\n",
    "        \n",
    "        \n",
    "        # mean and std deviation for DL\n",
    "        mean_train=pd.Series([8.604212, 1.166066,1.315720,3.054392,2.989617,0.241170,0.467873,0.322802,\n",
    "                              68.995504,21.356927,47.638576,0.490486,0.982286,1.325843,3.709488,9.053165,\n",
    "                              5.149527,0.002639,4.576947,2.015640,1.354386,0.001184], index= ['numericals', 'misc_ch', 'dash', 'dot', 'slash', 'quest', 'equal',\n",
    "           'underscore', 'length_url', 'domain_length', 'path_length',\n",
    "           'brandCount', 'suspiciousCount', 'comm_tldCount', 'no_tok_dom',\n",
    "           'dom_large_tok_len', 'avg_dom_tok_len', 'is_IP', 'avg_path_tok_len',\n",
    "           'dir_count','domain_digit_count', 'presence_http'])\n",
    "\n",
    "        std_dev_train=pd.Series([19.930884,4.801492,2.654109,2.307779,2.044907,0.521083,1.241500,1.033941,64.478629,20.941191,\n",
    "58.932598,0.771907,2.088172,1.112662,4.017686,4.397840,1.928341,0.051306,2.508113,2.026568,\n",
    "6.889827,0.034387],index= ['numericals', 'misc_ch', 'dash', 'dot', 'slash', 'quest', 'equal',\n",
    "           'underscore', 'length_url', 'domain_length', 'path_length',\n",
    "           'brandCount', 'suspiciousCount', 'comm_tldCount', 'no_tok_dom',\n",
    "           'dom_large_tok_len', 'avg_dom_tok_len', 'is_IP', 'avg_path_tok_len',\n",
    "           'dir_count','domain_digit_count', 'presence_http'])\n",
    "\n",
    "        vocab = {'\\t': 1,'\\n': 2, '\\x0b': 3, '\\x0c': 4, '\\r': 5, ' ': 6, '!': 7, '\"': 8, '#': 9, '$': 10, '%': 11, '&': 12,\n",
    "         \"'\": 13, '(': 14, ')': 15, '*': 16, '+': 17, ',': 18, '-': 19, '.': 20, '/': 21, '0': 22, '1': 23, '2': 24, '3': 25,\n",
    "                  '4': 26, '5': 27, '6': 28, '7': 29, '8': 30, '9': 31, ':': 32, ';': 33, '<': 34, '=': 35, '>': 36, '?': 37,\n",
    "         '@': 38, 'A': 39, 'B': 40, 'C': 41, 'D': 42, 'E': 43, 'F': 44, 'G': 45, 'H': 46, 'I': 47, 'J': 48, 'K': 49, 'L': 50,\n",
    "         'M': 51, 'N': 52, 'O': 53, 'P': 54, 'Q': 55, 'R': 56, 'S': 57, 'T': 58, 'U': 59, 'V': 60, 'W': 61, 'X': 62, 'Y': 63,\n",
    "         'Z': 64, '[': 65, '\\\\': 66, ']': 67, '^': 68, '_': 69, '`': 70, 'a': 71, 'b': 72, 'c': 73, 'd': 74, 'e': 75, 'f': 76,\n",
    "         'g': 77, 'h': 78, 'i': 79, 'j': 80, 'k': 81, 'l': 82, 'm': 83, 'n': 84, 'o': 85, 'p': 86, 'q': 87, 'r': 88, 's': 89,\n",
    "         't': 90, 'u': 91, 'v': 92, 'w': 93, 'x': 94, 'y': 95, 'z': 96, '{': 97, '|': 98, '}': 99, '~': 100, 'UNK': 101}\n",
    "\n",
    "        max_length = 1000 \n",
    "        real_ip = (real_ip - mean_train) / std_dev_train\n",
    "        sequence = []\n",
    "        def seq1(url):\n",
    "            seq = []\n",
    "            encoded_seq = [vocab[k] for k in url]\n",
    "            seq.append(encoded_seq)\n",
    "            return seq\n",
    "        \n",
    "        p = seq1(url1)\n",
    "        p = pad_sequences(p, maxlen=max_length)\n",
    "            \n",
    "        y_prob = model.predict([real_ip,p]) \n",
    "        \n",
    "        if (y_prob > 0.5):\n",
    "            prediction = \"Phishing URL\"\n",
    "        else:\n",
    "            prediction = \"Legitimate URL\" \n",
    "        a = \"The URL format is correct\" \n",
    "            \n",
    "        \n",
    "        return render_template(\"index.html\", prediction=prediction,url=url, x=x,m=m)      \n",
    "    \n",
    "@app.route(\"/aboutus\")\n",
    "def aboutus():\n",
    "    return render_template(\"aboutus.html\")\n",
    "\n",
    "@app.route(\"/Documentation\")\n",
    "def Documentation():\n",
    "    return render_template(\"A.html\")    \n",
    "\n",
    "@app.route(\"/graphs\")\n",
    "def graphs(): \n",
    "    return render_template(\"graphs.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [12/Dec/2021 18:00:54] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:44:01] \"\u001b[37mGET /Documentation HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:44:21] \"\u001b[37mGET /aboutus HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:44:23] \"\u001b[37mGET /Documentation HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:44:41] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:44:45] \"\u001b[37mGET /Documentation HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:45:01] \"\u001b[37mGET /aboutus HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:45:09] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb2ea64430> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Dec/2021 18:46:19] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb2d9ee5e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Dec/2021 18:47:40] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7feb2ea64280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [12/Dec/2021 18:49:10] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [12/Dec/2021 18:53:36] \"\u001b[37mPOST / HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
